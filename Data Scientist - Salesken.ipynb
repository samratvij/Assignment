{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Assignment\n",
    ">Salesken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement - 1: Matching the misspelt cities.\n",
    "\n",
    "There are two data files (.csv):\n",
    "<ol>\n",
    "<li>Correct_cities.csv : This file consists of a list of cities and they are spelt correctly. It has three columns \"name\" which is a city name; \"country\" where the city belongs to and \"id\" which is a unique id for each city.</li>\n",
    "\n",
    "<li>Misspelt_cities.csv : This file consists of city names which are mispelt. Each city name has been misspelt by randomly replacing 20% of the characters by some random characters. It has two columns \"misspelt_name\" and \"country\".</li>\n",
    "</ol>\n",
    "\n",
    "__Question:__  Write an algorithm or a group of algorithms to match the misspelt city name with the correct city name and return the list of unique ids for each misspelt city name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution Approach:__<br>\n",
    "- To find the perfect match of correct city name for every misspelt name, we will find the Hamming distance between misspelt and correct city names.<br>\n",
    "- Since we have country name column in misspelt data, we won't need to find the distance for every combination of correct and misspelt city name. We will join both datasets on country and find the minimum Hamming distance for every country-correct_city_name combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textdistance\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/18/31397b687f50ffae65469175f07faa68f288e27fcd8716276004c42e5637/textdistance-4.1.5-py3-none-any.whl\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textdistance import hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "correct_cities = pd.read_csv('https://raw.githubusercontent.com/SuvroBaner/SaleskenProblemSolving/master/Correct_cities.csv')\n",
    "misspelt_cities = pd.read_csv('https://raw.githubusercontent.com/SuvroBaner/SaleskenProblemSolving/master/Misspelt_cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               name               country       id\n",
      "0      les Escaldes               Andorra  3040051\n",
      "1  Andorra la Vella               Andorra  3041563\n",
      "2    Umm al Qaywayn  United Arab Emirates   290594\n",
      "3    Ras al-Khaimah  United Arab Emirates   291074\n",
      "4      Khawr Fakkān  United Arab Emirates   291696 \n",
      "\n",
      "    misspelt_name        country\n",
      "0  Hfjdúszoposzló        Hungary\n",
      "1        Otrajnyy         Russia\n",
      "2      ian Isidre           Peru\n",
      "3   Bordj Zemoufa        Algeria\n",
      "4     ChulamViwta  United States\n"
     ]
    }
   ],
   "source": [
    "print(correct_cities.head(),'\\n')\n",
    "print(misspelt_cities.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "country    0\n",
      "id         0\n",
      "dtype: int64 \n",
      "\n",
      "misspelt_name    0\n",
      "country          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking nulls in dataset\n",
    "print(correct_cities.isnull().sum(),'\\n')\n",
    "print(misspelt_cities.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23018 \n",
      "\n",
      "23018\n"
     ]
    }
   ],
   "source": [
    "#Checking length of datasets\n",
    "print(len(correct_cities),'\\n')\n",
    "print(len(misspelt_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 \n",
      "\n",
      "244 \n",
      "\n",
      "244\n"
     ]
    }
   ],
   "source": [
    "#Checking number of unique countries in both datasets and finding number of common countries\n",
    "print(len(correct_cities['country'].unique()),'\\n')\n",
    "print(len(misspelt_cities['country'].unique()),'\\n')\n",
    "print(len(correct_cities[['country']].merge(correct_cities[['country']], on = 'country', how = 'outer').drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "244 Countries are present in both datasets and all are common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with only one city:  54 \n",
      "\n",
      "Countries with 5 or more cities:  164\n"
     ]
    }
   ],
   "source": [
    "#Checking number of cities in each country\n",
    "##If there are many countries with only 1 city, we can separate them out and calculate distance for the remaining countries only\n",
    "country_size = pd.DataFrame(correct_cities.groupby(['country']).size()).reset_index()\n",
    "country_size.columns = ['country','size']\n",
    "print('Countries with only one city: ',len(country_size.loc[country_size['size'] == 1]),'\\n')\n",
    "print('Countries with 5 or more cities: ',len(country_size.loc[country_size['size'] >= 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>id</th>\n",
       "      <th>misspelt_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>3040051</td>\n",
       "      <td>les vsualdes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>3040051</td>\n",
       "      <td>Andopma ll Vella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>3041563</td>\n",
       "      <td>les vsualdes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>3041563</td>\n",
       "      <td>Andopma ll Vella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>290594</td>\n",
       "      <td>Dibka fl-Hisn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name               country       id     misspelt_name\n",
       "0      les Escaldes               Andorra  3040051      les vsualdes\n",
       "1      les Escaldes               Andorra  3040051  Andopma ll Vella\n",
       "2  Andorra la Vella               Andorra  3041563      les vsualdes\n",
       "3  Andorra la Vella               Andorra  3041563  Andopma ll Vella\n",
       "4    Umm al Qaywayn  United Arab Emirates   290594     Dibka fl-Hisn"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging both datasets on country to get every correct-misspelt_city name combination at country level\n",
    "base_df = correct_cities.merge(misspelt_cities, on = 'country', how = 'inner')\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%decrease in number of rows =  89.53 % /n\n",
      "New number of rows: 2246300\n"
     ]
    }
   ],
   "source": [
    "#Filtering out only those rows where length of string name and misspelt_name is equal\n",
    "base_df2 = base_df.loc[base_df['name'].str.len() == base_df['misspelt_name'].str.len()]\n",
    "print('%decrease in number of rows = ', round((len(base_df) - len(base_df2))*100/len(base_df),2),'% /n')\n",
    "print('New number of rows:', len(base_df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>id</th>\n",
       "      <th>misspelt_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>3040051</td>\n",
       "      <td>les vsualdes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>3041563</td>\n",
       "      <td>Andopma ll Vella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>290594</td>\n",
       "      <td>Umm al oaywaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>290594</td>\n",
       "      <td>Ras al-Khaamdh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ras al-Khaimah</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>291074</td>\n",
       "      <td>Umm al oaywaan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name               country       id     misspelt_name\n",
       "0      les Escaldes               Andorra  3040051      les vsualdes\n",
       "1  Andorra la Vella               Andorra  3041563  Andopma ll Vella\n",
       "2    Umm al Qaywayn  United Arab Emirates   290594    Umm al oaywaan\n",
       "3    Umm al Qaywayn  United Arab Emirates   290594    Ras al-Khaamdh\n",
       "4    Ras al-Khaimah  United Arab Emirates   291074    Umm al oaywaan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resetting index for dataset\n",
    "base_df2.reset_index(inplace=True)\n",
    "del base_df2['index']\n",
    "base_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>id</th>\n",
       "      <th>misspelt_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>290594</td>\n",
       "      <td>Umm al oaywaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>290594</td>\n",
       "      <td>Ras al-Khaamdh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ras al-Khaimah</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>291074</td>\n",
       "      <td>Umm al oaywaan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ras al-Khaimah</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>291074</td>\n",
       "      <td>Ras al-Khaamdh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dubai</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>292223</td>\n",
       "      <td>Dubai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245646</th>\n",
       "      <td>Beitbridge</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>895269</td>\n",
       "      <td>Beitbritje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245647</th>\n",
       "      <td>Beitbridge</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>895269</td>\n",
       "      <td>Ztishavaue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245648</th>\n",
       "      <td>Epworth</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1085510</td>\n",
       "      <td>Binduoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245649</th>\n",
       "      <td>Epworth</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1085510</td>\n",
       "      <td>Esworth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245650</th>\n",
       "      <td>Epworth</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1085510</td>\n",
       "      <td>Chezutu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2245651 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name               country       id   misspelt_name\n",
       "0        Umm al Qaywayn  United Arab Emirates   290594  Umm al oaywaan\n",
       "1        Umm al Qaywayn  United Arab Emirates   290594  Ras al-Khaamdh\n",
       "2        Ras al-Khaimah  United Arab Emirates   291074  Umm al oaywaan\n",
       "3        Ras al-Khaimah  United Arab Emirates   291074  Ras al-Khaamdh\n",
       "4                 Dubai  United Arab Emirates   292223           Dubai\n",
       "...                 ...                   ...      ...             ...\n",
       "2245646      Beitbridge              Zimbabwe   895269      Beitbritje\n",
       "2245647      Beitbridge              Zimbabwe   895269      Ztishavaue\n",
       "2245648         Epworth              Zimbabwe  1085510         Binduoa\n",
       "2245649         Epworth              Zimbabwe  1085510         Esworth\n",
       "2245650         Epworth              Zimbabwe  1085510         Chezutu\n",
       "\n",
       "[2245651 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering out cities where we are able to identify correct city names by just comparing len of name and misspelt_name\n",
    "\n",
    "#Getting country size for base_df2\n",
    "city_size_map = pd.DataFrame(base_df2.groupby(['id']).size()).reset_index()\n",
    "city_size_map.columns = ['id','size']\n",
    "\n",
    "#Filtering out cities with only one record in base_df2\n",
    "city_size_map = pd.DataFrame(base_df2.groupby(['id']).size()).reset_index()\n",
    "city_size_map.columns = ['id','size']\n",
    "city_size_map = city_size_map[city_size_map['size'] == 1]\n",
    "\n",
    "#Filtering out country names from base_df2 based on last result\n",
    "mapped_cities1 = base_df2.loc[base_df2['id'].isin(city_size_map['id'])]\n",
    "\n",
    "#Remaining unmapped dataset\n",
    "base_df3 = base_df2[~base_df2['id'].isin(mapped_cities1['id'])].reset_index()\n",
    "del base_df3['index']\n",
    "base_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a copy of base_df3\n",
    "base_df4 = base_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculating hamming distance between all rows of name and misspelt_name \n",
    "base_df4['distance'] = 0\n",
    "base_df4.loc[:,'distance'] = base_df4.loc[:, [\"name\",\"misspelt_name\"]].apply(lambda x: hamming(*x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>id</th>\n",
       "      <th>misspelt_name</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>290594</td>\n",
       "      <td>Umm al oaywaan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>290594</td>\n",
       "      <td>Ras al-Khaamdh</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ras al-Khaimah</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>291074</td>\n",
       "      <td>Umm al oaywaan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ras al-Khaimah</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>291074</td>\n",
       "      <td>Ras al-Khaamdh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dubai</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>292223</td>\n",
       "      <td>Dubai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245646</th>\n",
       "      <td>Beitbridge</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>895269</td>\n",
       "      <td>Beitbritje</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245647</th>\n",
       "      <td>Beitbridge</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>895269</td>\n",
       "      <td>Ztishavaue</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245648</th>\n",
       "      <td>Epworth</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1085510</td>\n",
       "      <td>Binduoa</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245649</th>\n",
       "      <td>Epworth</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1085510</td>\n",
       "      <td>Esworth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245650</th>\n",
       "      <td>Epworth</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1085510</td>\n",
       "      <td>Chezutu</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2245651 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name               country       id   misspelt_name  \\\n",
       "0        Umm al Qaywayn  United Arab Emirates   290594  Umm al oaywaan   \n",
       "1        Umm al Qaywayn  United Arab Emirates   290594  Ras al-Khaamdh   \n",
       "2        Ras al-Khaimah  United Arab Emirates   291074  Umm al oaywaan   \n",
       "3        Ras al-Khaimah  United Arab Emirates   291074  Ras al-Khaamdh   \n",
       "4                 Dubai  United Arab Emirates   292223           Dubai   \n",
       "...                 ...                   ...      ...             ...   \n",
       "2245646      Beitbridge              Zimbabwe   895269      Beitbritje   \n",
       "2245647      Beitbridge              Zimbabwe   895269      Ztishavaue   \n",
       "2245648         Epworth              Zimbabwe  1085510         Binduoa   \n",
       "2245649         Epworth              Zimbabwe  1085510         Esworth   \n",
       "2245650         Epworth              Zimbabwe  1085510         Chezutu   \n",
       "\n",
       "         distance  \n",
       "0               2  \n",
       "1              11  \n",
       "2              10  \n",
       "3               2  \n",
       "4               0  \n",
       "...           ...  \n",
       "2245646         2  \n",
       "2245647         8  \n",
       "2245648         7  \n",
       "2245649         1  \n",
       "2245650         6  \n",
       "\n",
       "[2245651 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating mapped_cities dataset based on lowest value of hamming distance\n",
    "mapped_cities2 = base_df4.loc[base_df4.groupby(['name','id'])['distance'].idxmin(),:'misspelt_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>misspelt_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>3040051</td>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>les vsualdes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>3041563</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andopma ll Vella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>291696</td>\n",
       "      <td>Khawr Fakkān</td>\n",
       "      <td>Khapr xakkān</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>292231</td>\n",
       "      <td>Dibba Al-Fujairah</td>\n",
       "      <td>tibba wl-Fujairab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>292239</td>\n",
       "      <td>Dibba Al-Hisn</td>\n",
       "      <td>Dibka fl-Hisn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23013</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2508184</td>\n",
       "      <td>’Aïn el Bell</td>\n",
       "      <td>’Afn ez Bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23014</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2508180</td>\n",
       "      <td>’Aïn el Berd</td>\n",
       "      <td>’Aïn eluherd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23015</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2508152</td>\n",
       "      <td>’Aïn el Hammam</td>\n",
       "      <td>’Aïnoel nammam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23016</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2508130</td>\n",
       "      <td>’Aïn el Melh</td>\n",
       "      <td>’Aïn el Mclg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23017</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2508119</td>\n",
       "      <td>’Aïn el Turk</td>\n",
       "      <td>’Aïnael murk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23018 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    country       id               name      misspelt_name\n",
       "0                   Andorra  3040051       les Escaldes       les vsualdes\n",
       "1                   Andorra  3041563   Andorra la Vella   Andopma ll Vella\n",
       "2      United Arab Emirates   291696       Khawr Fakkān       Khapr xakkān\n",
       "3      United Arab Emirates   292231  Dibba Al-Fujairah  tibba wl-Fujairab\n",
       "4      United Arab Emirates   292239      Dibba Al-Hisn      Dibka fl-Hisn\n",
       "...                     ...      ...                ...                ...\n",
       "23013               Algeria  2508184       ’Aïn el Bell       ’Afn ez Bell\n",
       "23014               Algeria  2508180       ’Aïn el Berd       ’Aïn eluherd\n",
       "23015               Algeria  2508152     ’Aïn el Hammam     ’Aïnoel nammam\n",
       "23016               Algeria  2508130       ’Aïn el Melh       ’Aïn el Mclg\n",
       "23017               Algeria  2508119       ’Aïn el Turk       ’Aïnael murk\n",
       "\n",
       "[23018 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concating mapped_cities1 and mapped_cities2\n",
    "mapped_cities = mapped_cities1.append(mapped_cities2).reset_index()\n",
    "mapped_cities = mapped_cities[['country','id','name','misspelt_name']]\n",
    "mapped_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the final output to a scv file\n",
    "mapped_cities.to_csv('mapped_cities.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Problem Statement - 2: Find the Semantic Similarity\n",
    "\n",
    "<ul>\n",
    "<li>Part - 1: Given a list of sentences (list_of_setences.txt) write an algorithm which computes the semantic similarity and return the similar sentences together.\n",
    "\n",
    "Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between them is based on the likeness of their meaning or semantic content.\n",
    "    </li>\n",
    ">For example : \"Football is played in Brazil\" and \"Cricket is played in India\". Both these sentences are about sports so they will have a semantic similarity.\n",
    "<li>\n",
    "Part - 2: Extend the above algorithm in form of a REST API. The input parameter is a list of sentences (refer to the file list_of_setences.txt) and the response is a list of list with the similar sentences.\n",
    "    </li>\n",
    "\n",
    ">For example : Say there are 4 sentences as an input list - [\"Football is played in Brazil\" , \"Cricket is played in India\", \"Traveling is good for health\", \"People love traveling in winter\"]<br>\n",
    "Output : [[\"Football is played in Brazil\" , \"Cricket is played in India\"], [\"Traveling is good for health\", \"People love traveling in winter\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/09/ed/b59a2edde05b7f5755ea68648487c150c7c742361e9c8733c6d4ca005020/gensim-3.8.1-cp37-cp37m-win_amd64.whl (24.2MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.3.2)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.17.4)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/73/c1/c25300e0afbe36f550d82affc0d1705076e9ea909ef33e8dd1f7147df10b/boto3-1.12.0-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "Collecting botocore<1.16.0,>=1.15.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/ba/236f25b9200f0cda4842585205b566979484d38927a8a302cc5c1beea10c/botocore-1.15.0-py2.py3-none-any.whl (5.9MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.0->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.0->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-1.9.0-cp37-none-any.whl size=73092 sha256=9c44f225aab68548e858774414f94bcac332d647c058e2cf66bc64227f1dd4fd\n",
      "  Stored in directory: C:\\Users\\gargi\\AppData\\Local\\pip\\Cache\\wheels\\ab\\10\\93\\5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.12.0 botocore-1.15.0 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.3.3 smart-open-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading req packages\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "import pprint\n",
    "import nltk \n",
    "import string \n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "sentences = pd.read_csv('https://raw.githubusercontent.com/SuvroBaner/SaleskenProblemSolving/master/list_of_sentences', \n",
    "                               header = None)\n",
    "sentences.columns = ['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how are you doing ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the weather is awesome today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>baseball is played in the USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>there is a thunderstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>are you doing good ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The polar regions are melting\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nokia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cricket is a fun game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>the climate change is a problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sentences\n",
       "0                      good morning\n",
       "1               how are you doing ?\n",
       "2      the weather is awesome today\n",
       "3                           samsung\n",
       "4                    good afternoon\n",
       "5     baseball is played in the USA\n",
       "6          there is a thunderstorm \n",
       "7              are you doing good ?\n",
       "8    The polar regions are melting\"\n",
       "9                             apple\n",
       "10                            nokia\n",
       "11            cricket is a fun game\n",
       "12  the climate change is a problem"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe into list\n",
    "list_of_sentences = sentences['sentences'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = copy.deepcopy(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "\n",
    "- Converting all words to lowercase\n",
    "- Removing punctuation\n",
    "- Removing all stop words (common english words)\n",
    "- Tokenize - Break sentences into words\n",
    "- Stem - Getting root words for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['good morning', 'how are you doing ?',\n",
       "       'the weather is awesome today', 'samsung', 'good afternoon',\n",
       "       'baseball is played in the usa', 'there is a thunderstorm ',\n",
       "       'are you doing good ?', 'the polar regions are melting\"', 'apple',\n",
       "       'nokia', 'cricket is a fun game',\n",
       "       'the climate change is a problem'], dtype=object)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting text to lower_case\n",
    "for x in range(len(list_of_sentences)):\n",
    "    list_of_sentences[x] =  list_of_sentences[x].lower()\n",
    "\n",
    "list_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['good morning', 'how are you doing ',\n",
       "       'the weather is awesome today', 'samsung', 'good afternoon',\n",
       "       'baseball is played in the usa', 'there is a thunderstorm ',\n",
       "       'are you doing good ', 'the polar regions are melting', 'apple',\n",
       "       'nokia', 'cricket is a fun game',\n",
       "       'the climate change is a problem'], dtype=object)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator)\n",
    "\n",
    "for x in range(len(list_of_sentences)):\n",
    "    list_of_sentences[x] =  remove_punctuation(list_of_sentences[x])\n",
    "\n",
    "list_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of list_of_sentences\n",
    "list_of_sentences_copy = copy.deepcopy(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['good', 'morning']), list([]),\n",
       "       list(['weather', 'awesome', 'today']), list(['samsung']),\n",
       "       list(['good', 'afternoon']), list(['baseball', 'played', 'usa']),\n",
       "       list(['thunderstorm']), list(['good']),\n",
       "       list(['polar', 'regions', 'melting']), list(['apple']),\n",
       "       list(['nokia']), list(['cricket', 'fun', 'game']),\n",
       "       list(['climate', 'change', 'problem'])], dtype=object)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text): \n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\")) \n",
    "    word_tokens = nltk.tokenize.word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return filtered_text \n",
    "  \n",
    "for x in range(len(list_of_sentences_copy)):\n",
    "    list_of_sentences_copy[x] =  remove_stopwords(list_of_sentences_copy[x])\n",
    "\n",
    "list_of_sentences_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords using nltk library removes all words from sentence 2 and takes the meaning away from sentence 8.\n",
    "Hence, we will not use the stopwords from that library and remove stopwords manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['good', 'morning'],\n",
       " ['how', 'doing'],\n",
       " ['weather', 'awesome', 'today'],\n",
       " ['samsung'],\n",
       " ['good', 'afternoon'],\n",
       " ['baseball', 'played', 'usa'],\n",
       " ['thunderstorm'],\n",
       " ['doing', 'good'],\n",
       " ['polar', 'regions', 'melting'],\n",
       " ['apple'],\n",
       " ['nokia'],\n",
       " ['cricket', 'fun', 'game'],\n",
       " ['climate', 'change', 'problem']]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in is are you there'.split(' '))\n",
    "\n",
    "texts = [[word for word in document.split() if word not in stoplist]\n",
    "         for document in list_of_sentences]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "        #tokens = [word for word in nltk.word_tokenize(text) if len(word) > 1]  #tokenize\n",
    "        stems = [stemmer.stem(item) for item in text] #stem\n",
    "        return stems\n",
    "\n",
    "texts2 = []\n",
    "for x in range(len(texts)):\n",
    "    texts2.append(stem(texts[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['good', 'morn'],\n",
       " ['how', 'do'],\n",
       " ['weather', 'awesom', 'today'],\n",
       " ['samsung'],\n",
       " ['good', 'afternoon'],\n",
       " ['basebal', 'play', 'usa'],\n",
       " ['thunderstorm'],\n",
       " ['do', 'good'],\n",
       " ['polar', 'region', 'melt'],\n",
       " ['appl'],\n",
       " ['nokia'],\n",
       " ['cricket', 'fun', 'game'],\n",
       " ['climat', 'chang', 'problem']]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf=vectorizer.fit_transform(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.        , 0.        , 0.36899732,\n",
       "       0.        , 0.        , 0.40302781, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cosine Similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# cosine_similarities of element 1 with others\n",
    "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 'good morning' is showing good similarity with 'good afternoon' and 'are you doing good?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compute Cosine similarity for other elements of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than this, we can use word2vec (or doc2vec) and produce much better results using their pretrained corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
